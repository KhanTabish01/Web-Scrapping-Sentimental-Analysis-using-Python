{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = load_workbook('Output.xlsx')\n",
    "ws = wb.worksheets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2564555279.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\tkhan\\AppData\\Local\\Temp\\ipykernel_85536\\2564555279.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\Extracted Data\\\\{x}.txt',\"r\",encoding=\"ISO-8859-1\")\u001b[0m\n\u001b[1;37m                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "for x in range(1,151):\n",
    "    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\Extracted Data\\\\{x}.txt',\"r\",encoding=\"ISO-8859-1\")\n",
    "    sentence = f.read()\n",
    "    filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\n",
    "    words = [word for word in filtered.split() if word]\n",
    "    average = sum(map(len, words))/len(words)\n",
    "    ws[f\"O{x+1}\"] = round(average,2)\n",
    "    wb.save('Output.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,151):\n",
    "    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\Extracted Data\\\\{x}.txt',\"r\",encoding=\"ISO-8859-1\")\n",
    "    sentence = f.read()\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    pronouns = pronounRegex.findall(sentence)\n",
    "    ws[f\"N{x+1}\"] = len(pronouns)\n",
    "    wb.save('Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numSyllables(Well):\n",
    "    word = Well.lower()\n",
    "    Syllable = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    Sy=(len(Well))\n",
    "    if Sy <=3:\n",
    "        Syllable+= 1\n",
    "    for index in range(1,len(Well)):\n",
    "        if Well[index] in vowels and Well[index-1] not in vowels:\n",
    "            Syllable+=1\n",
    "            if Well.endswith (\"es\") and Well.endswith(\"ed\"):\n",
    "                Syllable -=1\n",
    "            elif Well.endswith (\"des\") and Well.endswith(\"tes\") and Well.endswith(\"ded\") and Well.endswith(\"ted\"):\n",
    "                Syllable +=1\n",
    "            elif Well.endswith (\"e\"):\n",
    "                Syllable -=1\n",
    "        if Syllable == 0:\n",
    "            Syllable +=1\n",
    "    return Syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for x in range(1,151):\n",
    "    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\Extracted Data\\\\{x}.txt',\"r\",encoding=\"ISO-8859-1\")\n",
    "    sentence = f.read()\n",
    "    filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\n",
    "    words = [word for word in filtered.split() if word]\n",
    "    for item in words:\n",
    "        count+= numSyllables(item)\n",
    "    ws[f\"M{x+1}\"] = count\n",
    "    wb.save('Output Data Structure.xlsx')\n",
    "    count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\tWord Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english')) \n",
    "words = []\n",
    "words_without_sw = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,151):\n",
    "    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\Extracted Data\\\\{x}.txt',\"r\",encoding=\"ISO-8859-1\")\n",
    "    sentence = f.read()\n",
    "    filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\n",
    "    words = [word for word in filtered.split() if word]\n",
    "    for r in words: \n",
    "        if not r in stop_words: \n",
    "            words_without_sw.append(r)\n",
    "    \n",
    "    ws[f\"L{x+1}\"] = len(words_without_sw)\n",
    "    wb.save('Output Data Structure.xlsx')\n",
    "    words_without_sw.clear() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "complex_word_count = 0\n",
    "for x in range(1,151):\n",
    "    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\Extracted Data\\\\{x}.txt',\"r\",encoding=\"ISO-8859-1\")\n",
    "    sentence = f.read()\n",
    "    filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\n",
    "    words = [word for word in filtered.split() if word]\n",
    "    for item in words:\n",
    "        count = numSyllables(item)\n",
    "        if count>2:\n",
    "            complex_word_count+=1\n",
    "    ws[f\"K{x+1}\"] = complex_word_count\n",
    "    wb.save('Output Data Structure.xlsx')\n",
    "    count = 0\n",
    "    complex_word_count = 0\n",
    "    words.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,151):\n",
    "    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\Extracted Data\\\\{x}.txt',\"r\",encoding=\"ISO-8859-1\")\n",
    "    sentence = f.read()\n",
    "    number_of_sentences = re.split(r'[.!?]+', sentence)\n",
    "    filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\n",
    "    words = [word for word in filtered.split() if word]\n",
    "    \n",
    "    ws[f\"J{x+1}\"] = round(len(words)/len(number_of_sentences),2)\n",
    "    wb.save('Output Data Structure.xlsx')\n",
    "    words.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage of Complex words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "complex_word_count = 0\n",
    "for x in range(1,151):\n",
    "    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\Extracted Data\\\\{x}.txt',\"r\",encoding=\"ISO-8859-1\")\n",
    "    sentence = f.read()\n",
    "    filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\n",
    "    words = [word for word in filtered.split() if word]\n",
    "    for item in words:\n",
    "        count = numSyllables(item)\n",
    "        if count>2:\n",
    "            complex_word_count+=1\n",
    "    ws[f\"H{x+1}\"] = round(complex_word_count/len(words),3)*100\n",
    "    wb.save('Output Data Structure.xlsx')\n",
    "    count = 0\n",
    "    complex_word_count = 0\n",
    "    words.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOG Index - calculated using excel function<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### =SUM(G2:H2)*0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_files= ['#','StopWords_Auditor','StopWords_Currencies','StopWords_DatesandNumbers','StopWords_Generic','StopWords_GenericLong','StopWords_Geographic','StopWords_Names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,8):\n",
    "    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\StopWords\\\\{stop_words_files[x]}.txt',\"r\",encoding=\"ISO-8859-1\")   \n",
    "    line = f.read()\n",
    "    stop_words_list = line.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_without_stopwords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a list of positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('C:\\Users\\tkhan\\Desktop\\SIP project\\\\MasterDictionary\\\\positive-words.txt',\"r\")\n",
    "lines = f1.read()\n",
    "positive_words = lines.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count = 0\n",
    "negative_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a list of negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('C:\\Users\\tkhan\\Desktop\\SIP project\\\\MasterDictionary\\\\negative-words.txt',\"r\")\n",
    "lines = f2.read()\n",
    "negative_words = lines.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,151):\n",
    "    f = open(f'C:\\Users\\tkhan\\Desktop\\SIP project\\\\Extracted Data\\\\{x}.txt',\"r\",encoding=\"ISO-8859-1\")\n",
    "    sentence = f.read()\n",
    "    filtered = ''.join(filter(lambda x: x not in '\".,;!-', sentence))\n",
    "    words = word_tokenize(filtered)\n",
    "    \n",
    "    for r in words: \n",
    "        if not r in stop_words_list: \n",
    "            words_without_stopwords.append(r)\n",
    "    \n",
    "    for word in words_without_stopwords:\n",
    "        if word in positive_words:\n",
    "            positive_count += 1\n",
    "        elif word in negative_words:\n",
    "            negative_count += 1\n",
    "            \n",
    "    ws[f\"C{x+1}\"] = positive_count\n",
    "    ws[f\"D{x+1}\"] = negative_count\n",
    "    wb.save('Output Data Structure.xlsx')\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    words_without_stopwords.clear()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
